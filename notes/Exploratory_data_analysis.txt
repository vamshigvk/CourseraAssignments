
Types of data analysis:

1.Descriptive : describe or summarize a set of data
2.Exploratory : examine the data and find relationships that weren't known previously. Correlation doesn't imply causation
3.Inferential : use a relatively small sample of data to say something about the population at large
4.Predective  : use current and historical data to make predictions about future data
5.Casual      : see what happens to one variable when we manipulate another variable
6.Mechanistic : understand the exact changes in variables that lead to exact changes in other variables.
________________________________________________________________________

Principles of analytic graphics:
principle 1 : show comparisions
principle 2 : show casuality, mechanism, explanation
principle 3 : show multivariate data
principle 4 : integrate multiple modes of evidence
principle 5 : describe and document the evidence
principle 6 : content is king
________________________________________________________________________

why do we use graphics in data analysis?:
-To understand data properties
-To find patterns in data
-To suggest modeling strategies
-To debug analyses
-To communicate results
________________________________________________________________________

x=rand(100)
hist(x)

y=rand(100)
plot(x,y)

example(points) is used to get examples for pch , 

segments(rep(1, 52), mrg[, 2], rep(2, 52), mrg[, 3]) # segments function is used to create like match the following or make lines between points

pch: means shape of points in par.(default is open circle) 
lty: linetype (fefault is solid line), can be dashed , dotted etc
lwd: line width 
col: color 
xlab: character string for x axis label
ylab: character string for y axis label

par() we can use these options : 
las: the orientation of the axis labels on the plot
bg: background color
mar: margin size
oma: the outer margin size
mfrow: no. of plots per row, column (row based insertion), similarly mfcol

mtext: gives the arbitrary text to the margins(inner or outer) of the plot
axis : adding axis ticks or labels 

mar is margin EX: par(mar=c(4,4,2,2))  #bottom  , left , top , right
title("scatterplot") gives title to plot
text(-2,-2, "Label") is used to add text inside the plot
legend("topleft", legend=data , pch=20)
legend("topright",c("Sub_metering_1","Sub_metering_2","Sub_metering_3"), lty=1, lwd=2.5, col = c("black" ,"red" ,"blue"))
fit <- lm(x ~ y)
abline(fit) is used to add linear model fit to the plot
abline(fit, lwd=3, col="blue") to increase the line width, col to add color to line.
rug(ppm) gives the detailed info of plot such as how many datapoints are in each bucket when hist(ppm) is used.

or add all options to plot itself as below
plot(x,y, xlab="Weight", ylab="height", main= "scatterplot", pch=20)

z=rpois(100,2)
par(mfrow=c(2,1)) to make  multiple plots on a page rowWise, and use mfcol to plot in columnWise
plot(x,y, pch=20)
plot(x,z,pch=19) 
plot(x,y, type="n") which makes a plot but doesnot put data into it.
points() is used to add points of data sequentially by group.
points(x[g=="male"],y[g=="male"])
points(x[g=="female"],y[g=="female"], col="pink")

#saving a plot as pdf to windows 
pdf(file="myplot.pdf")
plot(faithful$eruptions, faithful$waiting)
title("old geyser data")
dev.off()

featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot= "pairs")

2 types of graphic file devices:
vector: pdf, svg(scalable vector graphics), win.metafile, postscript
bitmap formats: png(portable network graphics), jpeg, tiff, bmp

dev.set(<integer>) is used to set graphic device, integer>=2

dev.copy(png, file="filename.png") and dev.off() or dev.cpoy2pdf() when you want to save a plot as pdf or png

the command dev.cur(). This will show you the current plotting device, the screen.

lattice plot system: all arguments for the plot should be given in single function only. you cannot "add" to the plot once it is created as you can with the base system.
Ex: xyplot() , bwplot()

p<-xyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))

mynames[myfull] gives non null values of names , p[["formula"]] gives Life.Exp ~ Income 

ggplot2() (grammer of graphics) allows you to annotate by adding to the plot (as base does)

ggsave(filename="exploratory_data_analysis_week4_project/plot3.png", plot=g)

So you can add text, title, points, and lines to an existing plot. To add lines, you give a vector of x values and a corresponding
| vector of y values (or a 2-column matrix); the function lines just connects the dots. The function text adds text labels to a plot
| using specified x, y coordinates.

| The function title adds annotations. These include x- and y- axis labels, title, subtitle, and outer margin. Two other annotating
| functions are mtext which adds arbitrary text to either the outer or inner margins of the plot and axis which adds axis ticks and
| labels. Another useful function is legend which explains to the reader what the symbols your plot uses mean.


mtext("Ozone and Weather in New York City", outer = TRUE )

use RColorBrewer package to play with colors in lattice plot.

colorRamp and colorRampPalette, give you more options. Both of these take color names as arguments and use them as "palettes", that
| is, these argument colors are blended in different proportions to form new colors. and colors() lists the names of colors you can use in any plotting function. 

>image(volcano, col = pal(20)) gives color to volcano dataset in an image.

pal <- colorRamp(c("red","blue")) , pal takes only 0 to 1 as input arg
> pal(0)
gives [RGB]
     [,1] [,2] [,3]
[1,]  255    0    0

pal(seq(0,1,len=6)) 
gives
[,1] [,2] [,3]
[1,]  255    0    0
[2,]  204    0   51
[3,]  153    0  102
[4,]  102    0  153
[5,]   51    0  204
[6,]    0    0  255

colorRampPalette:
p1 <- colorRampPalette(c("red", "blue"))   , The argument you pass to p1 specifies the number of colors you want returned.
> p1(2)
[1] "#FF0000" "#0000FF"    #first is red and second is blue. # 6 digits represents 2 chars of red + 2 chars of green + 2 chars of blue.

colorwheel - red - pink - blue - sky blue - green - yellow- red in clockwise

RColorBrewer Package, available on CRAN, that contains interesting and useful color palettes, of which there are 3 types, sequential, divergent, and qualitative
The top section shows the sequential palettes in which the colors are
| ordered from light to dark. The divergent palettes are at the bottom. Here the neutral color (white) is in the center, and as you
| move from the middle to the two ends of each palette, the colors increase in intensity. The middle display shows the qualitative
| palettes which look like collections of random colors. These might be used to distinguish factors in your data.

> library(RColorBrewer)
> cols= brewer.pal(3,"BuGn")
> cols
[1] "#E5F5F9" "#99D8C9" "#2CA25F"
> image(volcano, col = cols)

use smoothScatter if you have lot of data points to ignore overlapping, it uses blues color from brewer.
> x=rnorm(10000)
> y=rnorm(10000)
> smoothScatter(x,y)

rgb():rgb function takes arguments for red, green and blue, and what it returns. So these are numbers between zero and one for red, green and blue and it will return, hexadecimal string that can be passed to functions like image or plot.  
the alpha parameter, which have been used, to specify transparency in your colors. Zero being the most transparent, or basically completely transparent, and one being not transparent at all. 
plot(x,y,col=rgb(0,0,1,0.2),pch=19)

________________________________________________________________________

ggplot2 combines the best of base and lattice. It allows for multipanel (conditioning) plots (as lattice does) but also post
| facto annotation (as base does), so you can add titles and labels. It uses the low-level grid package (which comes with R) to draw
| the graphics. As part of its grammar philosophy, ggplot2 plots are composed of aesthetics (attributes such as size, shape, color) and
| geoms (points, lines, and bars), the geometric objects you see on the plot.

qplot(displ, hwy, data = mpg, color = drv, geom = c("point", "smooth"))
`geom_smooth()` using method = 'loess' and formula 'y ~ x'

qplot(drv, hwy, data = mpg, geom = "boxplot") to get boxplot, geom=c("boxplot","jitter") gives points on boxplot, grid.arrange(p1,p2,ncol=2)

> qplot(hwy, data = mpg, facets = drv ~ . , binwidth = 2) to get histogram

qplot(hwy, data = mpg,fill = drv) instead of colors , to fill with colors.

basic components(7) of ggplot2: there's a DATA FRAME which contains the data you're trying to plot. Then the AESTHETIC MAPPINGS determine how data are
| mapped to color, size, etc. The GEOMS (geometric objects) are what you see in the plot (points, lines, shapes) and FACETS are the
| panels used in conditional plots.
STATS are statistical transformations such as binning, quantiles, and smoothing which ggplot2 applies to the data.
| SCALES show what coding an aesthetic map uses (for example, male = red, female = blue). Finally, the plots are depicted on a
| COORDINATE SYSTEM. When you use qplot these were taken care of for you.

"artist's palette" model means in the context of plotting? : plots are built up in layers

 g <- ggplot(mpg, aes(displ, hwy))
 summary(g) gives what default values all it contains
 
  Note that if you tried to print g with the expressions g or print(g) you'd get an error! Even though it's a great package, ggplot
| doesn't know how to display the data yet since you didn't specify how you wanted to see it. Now type g+geom_point() and see what
| happens.
 g + geom_point() gives you a scattered plot

> g + geom_point() + geom_smooth() gives default smoothing with confidence band.

> g + geom_point() + geom_smooth(method = "lm") gives regression line through data.

> g + geom_point() + geom_smooth(method = "lm") + facet_grid(. ~ drv ) to make panels w.r.t drv. or divide graph as per factors.

You can add your own annotation using functions such as xlab(), ylab(), and ggtitle(). 

> g + geom_point(color = "pink", size= 4, alpha = 1/2) is used to modify aesthetics, means points in the plot. all are constants in it.

> g + geom_point( size= 4, alpha = 1/2, aes(color = drv)) # use aes to add data to geom_point

> g + geom_point(aes(color = drv)) + labs( title = "Swirl Rules!") + labs( x= "Displacement" , y = "Hwy Mileage") #to add title , xlab and ylab

> g + geom_point(aes(color = drv)) + labs( title = "Swirl Rules!",  x= "Displacement" , y = "Hwy Mileage") #or you can combine labs

> g + geom_point(aes(color= drv), size=2, alpha=0.5) +geom_smooth(size = 4, linetype= 3, method = "lm", se = FALSE) 
The method specified a linear regression (note the negative slope indicating that the bigger the
| displacement the lower the gas mileage), the linetype specified that it should be dashed (not continuous), the size made the dashes
| big, and the se flag told ggplot to turn off the gray shadows indicating standard errors (confidence intervals)

> g + geom_point(aes(color= drv)) + theme_bw(base_family = "Times")
| No more gray background! Also, if you have good eyesight, you'll notice that the font in the labels changed.

> plot(myx, myy, type = "l", ylim = c(-3,3)) 
 The outlier at (50,100) is NOT shown on the line plot. Now we'll
| plot the same data with ggplot. Recall that the name of the dataframe is testdat. Create the graphical object g with a call to ggplot
| with 2 arguments,

> g <- ggplot(testdat, aes(myx, myy))
> g + geom_line()
| Notice how ggplot DID display the outlier point at (50,100). As a result the rest of the data is smashed down so you don't get to see
| what the bulk of it looks like.

> g + geom_line() + ylim(-3,3)
 ggplot simply ignored the outlier point at (50,100). There's a break in the line which isn't very noticeable.

> g + geom_line() + coord_cartesian( ylim=c(-3,3))
This looks more like the plot produced by the base plot function. The outlier y value at x=50 is not shown, but the plot indicates that it is larger than 3.
 
> g + geom_point() +facet_grid(drv~cyl, margins = TRUE)
The margins argument tells ggplot to display the marginal totals over each row and column, so instead of seeing 3
| rows (the number of drv factors) and 4 columns (the number of cyl factors) we see a 4 by 5 display. Note that the panel in position
| (4,5) is a tiny version of the scatterplot of the entire dataset.

> g + geom_point() +facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method = "lm", size =2, se=FALSE, color = "black")

g + geom_point() +facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method = "lm", size =2, se=FALSE, color = "black") + labs(x = "Displacement" , y ="Highway Mileage", title= "Swirl Rules!")

> qplot(carat, price, data = diamonds, shape = cut)
the cuts of the diamonds are distinguished by different symbols. The legend at the right tells you which symbol is associated with each cut.
> qplot(carat, price, data = diamonds, color = cut) #use color instead shape in this case to distinguish easily

use below code to make diamonds$carat to factor from decimal
> cutpoints <- quantile(diamonds$carat, seq(0,1, length = 4) , na.rm=TRUE)
>diamonds$car2 <- cut(diamonds$carat ,cutpoints)
now diamonds$car2 is factor format of diamonds$carat. Now we can continue with our multi-facet plot

________________________________________________________________________

Hierarchical clustering:
each observation starts in its own cluster,and pairs of clusters are merged as one moves up the hierarchy.
one can decide to stop clustering either when the clusters are too far apart to be merged (distance criterion) or when there is a sufficiently small number of clusters (number criterion)."

An agglomerative approach:
-find closest 2 things
-put them together
-find next closest, make groups and then make clusters

Requires
-a defined distance
-a merging approach

Produces
-Dendrogram: a tree showing how close things are to each other

How do we define close?
-Most important step
	-Garbage in -> garbage out

Distance or similarity
1.Euclidean distance : continous  - root((x1-x2)^2 + (y1-y2)^2) and so on for high dimensional problems
2.Correlation : continous
3.Manhattan : Binary : imagine the path as a grid and calculate as |A1-A2|+|B1-B2|+...+|Z1-Z2| like Manhattan city of NewYork

cex	number indicating the amount by which plotting text and symbols should be scaled relative to the default. 1=default, 1.5 is 50% larger, 0.5 is 50% smaller, etc.
text(x+0.05, y+0.05, labels = as.character(1:12))

set.seed(1234)
par(mar=c(0,0,0,0))
x=rnorm(12,mean=rep(1:3, each=4),sd=0.2 )
y=rnorm(12,mean=rep(c(1,2,1),4),sd=0.2)
plot(x,y,col="blue", cex=2)
text(x+0.05, y+0.05, labels = as.character(1:12))

dataFrame=data.frame(x=x,y=y)
distxy=dist(dataFrame) calculates distance between all the different rows in dataFrame, and gives you distance matrix. defaultly chooses euclidean distance.
hClustering=hCluster(distxy)
plot(hClustering)
plot(as.dendrogram(hClustering) gives all leaves in same level

merging approaches:
1. complete linkage: the distance between those two clusters is the equal to the distance between the two farthest points.
2. average linkage: gives you the distance between the two centers of gravity. So that's the difference between the two pluses here

heatmap() function is used to visualize matrix data. if you have an extremely large table or a, a large matrix of numbers that are kind of similarly scaled, and you want to, just take a look at them really quickly in an organized way, you can call the heatmap function.And what the heat map function does is essentially runs a hierarchical cluster analysis on the rows of the table and on the columns of the table. gives an idea of the relationships between variables  or observations.
heat map : is "a graphical representation of data where the individual values contained in a matrix are represented as colors. ... Heat maps originated in 2D displays of the values in a data matrix. Larger values were represented by small dark gray or black squares (pixels) and smaller values by lighter squares."
________________________________________________________________________

K-Means clustering:
how do we define close?
how do we group things?
how do we visualize the grouping?
how do we interpret the grouping?

A partioning approach:
-fix a number of clusters
-Get "Centroids" of each cluster
-Assign things to closest centroid
-Recalculate centroids

Requires
-A defined distance metric
-A number of clusters
-An initial guess as to cluster centroids

Produces
-Final estimate of cluster centroids
-An assignment of each point to clusters

> kmeansObject = kmeans(dataFrame, centers = 3)
> kmeansObject
K-means clustering with 3 clusters of sizes 5, 3, 4

Cluster means:
          x        y
1 2.4991376 0.934185
2 2.2936867 2.097619
3 0.8904553 1.256871

Clustering vector:
 [1] 3 3 3 3 2 1 1 2 1 1 2 1

Within cluster sum of squares by cluster:
[1] 0.9393028 0.8018648 1.1039253
 (between_SS / total_SS =  75.8 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"     

plotting the kmeans clusters and centroids on plot:
par(mar=c(0.2,0.2,0.2,0.2))
plot(x,y, col=kmeansObject$cluster, pch=19, cex=2)
points(kmeansObject$centers, col=1:3, pch=3, cex=3, lwd=3)

Notes:
K-means requires a number of clusters
-pick by eye/intuition
-pick by cross validation/information theory etc.

k-means is not deterministic
-Different # of clusters
-Different number of iterations

>distTmp <- mdist(x,y,cx,cy) # x,y are points, cx,cy are initial centroids of clusters. 
         [,1]      [,2]      [,3]     [,4]      [,5]      [,6]      [,7]     [,8]      [,9]     [,10]     [,11]     [,12]
[1,] 1.392885 0.9774614 0.7000680 1.264693 1.1894610 1.2458771 0.8113513 1.026750 4.5082665 4.5255617 4.8113368 4.0657750
[2,] 1.108644 0.5544675 0.3768445 1.611202 0.8877373 0.7594611 0.7003994 2.208006 1.1825265 1.0540994 1.2278193 1.0090944
[3,] 3.461873 2.3238956 1.7413021 4.150054 0.3297843 0.2600045 0.4887610 1.337896 0.3737554 0.4614472 0.5095428 0.2567247

> apply(distTmp,2,which.min) #gives us the minimum dist cluster to each point.
 [1] 2 2 2 1 3 3 3 1 3 3 3 3
________________________________________________________________________

Dimension reduction:

Related Problems:
you have multivariate variables X1,X2,...Xn so X1=(X11...X1m)
-Find a new set of multivariate variables that are uncorrelated and explains as much variance as possible.
-If you put all the variables together in one matrix, find the best matrix created with fewer variables(lower rank) that explains the original data.	

The first goal is statistical(Solved by PCA) and the second goal is data compression(Solved by SVD)

Related Solutions:
Principal Components Analysis(PCA) and Singular value decomposition(SVD):
alternatives are:Factor analysis, Independent components analysis, Latent semantic analysis

SVD: If X is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"
	
	X = (U)(D)(V^T)

where the columns of U are Orthogonal(left singular vectors), the columns of V are Orthogonal(right singular vectors) and D is a diagonal matrix(singular values)

PCA: The principal components are equal to the right singular values if you first scale(subtract the mean, divide by the standard deviation) the variables
Principal Component Analysis, "a simple, non-parametric method for extracting relevant information from confusing data sets."
Basically, PCA is a method to reduce a high-dimensional data set to its essential elements (not lose information) and explain the variability in the data.but you should know that SVD and PCA are closely related. 
________________________________________________________________________

let mat be:
> mat
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    5    7

> svd(mat)
$d
[1] 9.5899624 0.1806108

$u
           [,1]       [,2]
[1,] -0.3897782 -0.9209087
[2,] -0.9209087  0.3897782

$v
           [,1]       [,2]
[1,] -0.2327012 -0.7826345
[2,] -0.5614308  0.5928424
[3,] -0.7941320 -0.1897921

>matu %*% diag %*% t(matv) #gives you back mat
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    5    7
________________________________________________________________________

> scale(mat)
           [,1]       [,2]       [,3]
[1,] -0.7071068 -0.7071068 -0.7071068
[2,]  0.7071068  0.7071068  0.7071068
attr(,"scaled:center")
[1] 1.5 3.5 5.0
attr(,"scaled:scale")
[1] 0.7071068 2.1213203 2.8284271

> svd(scale(mat))
$d
[1] 1.732051 0.000000

$u
           [,1]      [,2]
[1,] -0.7071068 0.7071068
[2,]  0.7071068 0.7071068

$v
          [,1]       [,2]
[1,] 0.5773503 -0.5773503
[2,] 0.5773503  0.7886751
[3,] 0.5773503 -0.2113249

> prcomp(scale(mat)) #This will give you the principal components of mat.
Standard deviations (1, .., p=2):
[1] 1.732051 0.000000

Rotation (n x k) = (3 x 2):
           PC1        PC2
[1,] 0.5773503 -0.5773503
[2,] 0.5773503  0.7886751
[3,] 0.5773503 -0.2113249

Notice that the principal components of the scaled matrix, shown in the Rotation component of the prcomp output, ARE the columns of V, the right singular values. Thus, PCA of a scaled matrix yields the V matrix (right singular vectors) of the same scaled matrix.

________________________________________________________________________

Here's a display of these values (on the left). The first one (12.46) is significantly bigger than the others. Since we don't have any units specified, to the right we've plotted the proportion of the variance each entry represents. We see that the first entry accounts for about 40% of the variance in the data. This explains why the first columns of the U and V matrices respectively showed the distinctive patterns in the row and column means so clearly.

________________________________________________________________________

> dataMatrix= as.matrix(dataFrame)[sample(1:12),]
> svd=svd(scale(dataMatrix)) #gives u,d,v
$d
[1] 3.385435 3.246357

$u
             [,1]       [,2]
 [1,] -0.22017162  0.1235892
 [2,] -0.13975047 -0.2421704
 [3,] -0.46470027  0.2173893
 [4,]  0.41054869  0.4387502
 [5,]  0.12735450 -0.3632053
 [6,]  0.03706364  0.4739084
 [7,]  0.12971515 -0.3166175
 [8,] -0.18709689 -0.1816410
 [9,] -0.45730170  0.1093194
[10,]  0.45133895 -0.0342142
[11,]  0.24665915  0.1686165
[12,]  0.06634086 -0.3937246

$v
          [,1]       [,2]
[1,] 0.7071068 -0.7071068
[2,] 0.7071068  0.7071068

par(mar=c(4,4,4,4))
> plot(svd$u[,1],12:1, , xlab = "Row", ylab = "First left singular vector",pch=19)
> plot(svd$v[,1], xlab = "Column", ylab = "First Right singular vector",pch=19,xlim = c(0,1))

plot(data,mgp=c(6,1,0)) #mpg is used to maintain distance between xlab/ylab and point labels on x-axis/y-axis

You just can't run it on a data set that has missing values.Before you run an svd or a pca you have to do below operations about the missing values:
This uses the k nearest neighbors to calculate a values to use in place of  the missing data. You may want to specify an integer k which indicates how many neighbors you want to average to create this replacement value.

library(impute) #from bioconductor.org
dataFrame2=impute.knn(dataFrame)$data
now plot First singular vectors.
________________________________________________________________________

Final Example:

Consider this low resolution image file showing a face. We'll use SVD and see how the first several components contain most of the information in the file so that storing a huge matrix might not be necessary.

...

  |===================================================                   |  73%
| The image data is stored in the matrix faceData. Run the R command dim on
| faceData to see how big it is.

> dim(faceData)
[1] 32 32

| You are amazing!

  |====================================================                  |  75%
| So it's not that big of a file but we want to show you how to use what you
| learned in this lesson. We've done the SVD and stored it in the object svd1
| for you. Here's the plot of the variance explained.

...

  |=====================================================                 |  76%
| According to the plot what percentage of the variance is explained by the
| first singular value?

1: 15
2: 100
3: 23
4: 40

Selection: 2

| Nice try, but that's not exactly what I was hoping for. Try again.

| At what height does the first (leftmost) point fall in the plot?

1: 23
2: 100
3: 40
4: 15

Selection: 3

| You got it right!

  |======================================================                |  77%
| So 40% of the variation in the data matrix is explained by the first
| component, 22% by the second, and so forth. It looks like most of the
| variation is contained in the first 10 components. How can we check this out?
| Can we try to create an approximate image using only a few components?

...

  |=======================================================               |  78%
| Recall that the data matrix X is the product of 3 matrices, that is X=UDV^t.
| These are precisely what you get when you run svd on the matrix X.

...

  |========================================================              |  80%
| Suppose we create the product of pieces of these, say the first columns of U
| and V and the first element of D. The first column of U can be interpreted as
| a 32 by 1 matrix (recall that faceData was a 32 by 32 matrix), so we can
| multiply it by the first element of D, a 1 by 1 matrix, and get a 32 by 1
| matrix result. We can multiply that by the transpose of the first column of
| V, which is the first principal component. (We have to use the transpose of
| V's column to make it a 1 by 32 matrix in order to do the matrix
| multiplication properly.)

...

  |=========================================================             |  81%
| Alas, that is how we do it in theory, but in R using only one element of d
| means it's a constant. So we have to do the matrix multiplication with the
| %*% operator and the multiplication by the constant (svd1$d[1]) with the
| regular multiplication operator *.

...

  |=========================================================             |  82%
| Try this now and put the result in the variable a1. Recall that svd1$u,
| svd1$d, and svd1$v contain all the information you need. NOTE that because of
| the peculiarities of R's casting, if you do the scalar multiplication with
| the * operator first (before the matrix multiplication with the %*% operator)
| you MUST enclose the 2 arguments (svd1$u[,1] and svd1$d[1]) in parentheses.


> a1 <-(svd1$u[,1] * svd1$d[1]) %*% t(svd1$v[,1])

| Perseverance, that's the answer.

  |==========================================================            |  83%
| Now to look at it as an image. We wrote a function for you called myImage
| which takes a single argument, a matrix of data to display using the R
| function image. Run it now with a1 as its argument.

> image(a1)

| Almost! Try again. Or, type info() for more options.

| Type myImage(a1) at the command prompt.

> myImage(a1)

| All that practice is paying off!

  |===========================================================           |  84%
| It might not look like much but it's a good start. Now we'll try the same
| experiment but this time we'll use 2 elements from each of the 3 SVD terms.

...

  |============================================================          |  86%
| Create the matrix a2 as the product of the first 2 columns of svd1$u, a
| diagonal matrix using the first 2 elements of svd1$d, and the transpose of
| the first 2 columns of svd1$v. Since all of your multiplicands are matrices
| you have to use only the operator %*% AND you DON'T need parentheses. Also,
| you must use the R function diag with svd1$d[1:2] as its sole argument to
| create the proper diagonal matrix. Remember, matrix multiplication is NOT
| commutative so you have to put the multiplicands in the correct order. Please
| use the 1:2 notation and not the c(m:n), i.e., the concatenate function, when
| specifying the columns.

> a2 <-svd1$u[,1:2] %*% diag(svd1$d[1:2]) %*% t(svd1$v[,1:2])

| You nailed it! Good job!

  |=============================================================         |  87%
| Use myImage again to see how a2 displays.

> myImage(a2)

| You are really on a roll!

  |==============================================================        |  88%
| We're starting to see slightly more detail, and maybe if you squint you see a
| grimacing mouth. Now let's see what image results using 5 components. From
| our plot of the variance explained 5 components covered a sizeable percentage
| of the variation. To save typing, use the up arrow to recall the command
| which created a2 and replace the a2 and assignment arrow with the call to
| myImage, and change the three occurrences of 2 to 5.

> myImage(svd1$u[,1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[,1:5]))

| Keep working like that and you'll get there!

  |==============================================================        |  89%
| Certainly much better. Clearly a face is appearing with eyes, nose, ears, and
| mouth recognizable. Again, use the up arrow to recall the last command
| (calling myImage with a matrix product argument) and change the 5's to 10's.
| We'll see how this image looks.

> myImage(svd1$u[,1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[,1:10]))

| You're the best!

  |===============================================================       |  90%
| Now that's pretty close to the original which was low resolution to begin
| with, but you can see that 10 components really do capture the essence of the
| image. Singular value decomposition is a good way to approximate data without
| having to store a lot.

...

  |================================================================      |  92%
| We'll close now with a few comments. First, when reducing dimensions you have
| to pay attention to the scales on which different variables are measured and
| make sure that all your data is in consistent units. In other words, scales
| of your data matter. Second, principal components and singular values may mix
| real patterns, as we saw in our simple 2-pattern example, so finding and
| separating out the real patterns require some detective work. Let's do a
| quick review now.

...

  |=================================================================     |  93%
| Which of the following cliches LEAST captures the essence of dimension
| reduction?

1: find the needle in the haystack
2: see the forest through the trees
3: a face that could launch a 1000 ships
4: separate the wheat from the chaff

Selection: 1

| Almost! Try again.

| Which choice fails to deal with discerning differences between the valuable
| and the invaluable.

1: separate the wheat from the chaff
2: a face that could launch a 1000 ships
3: find the needle in the haystack
4: see the forest through the trees

Selection: 2

| Keep up the great work!

  |==================================================================    |  94%
| A matrix X has the singular value decomposition UDV^t. The principal
| components of X are ?

1: the rows of V
2: the columns of U
3: the columns of V
4: the rows of U

Selection: 3

| Perseverance, that's the answer.

  |===================================================================   |  95%
| A matrix X has the singular value decomposition UDV^t. The singular values of
| X are found where?

1: the diagonal elements of D
2: the columns of U
3: the columns of D
4: the columns of V

Selection: 2

| Not quite right, but keep trying.

| Recall that U and V give us vectors and D gave us values.

1: the diagonal elements of D
2: the columns of U
3: the columns of V
4: the columns of D

Selection: 1

| You are amazing!

  |===================================================================   |  96%
| True or False? PCA and SVD are totally unrelated.

1: False
2: True

Selection: 1

| Keep up the great work!

  |====================================================================  |  98%
| True or False? D gives the singular values of a matrix in decreasing order of
| weight.

1: False
2: True

Selection: 2

| Nice work!


________________________________________________________________________

histogram with distribution curve: or density plot:

	d <- density(mtcars$mpg) # returns the density data
	plot(d) # plots the results
	polygon(d, col="red", border="blue") # Filled Density Plot